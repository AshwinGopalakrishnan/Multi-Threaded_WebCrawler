# Multi-Threaded WebCrawler Operating Systems Project

### Group Members: Daniel Shin, Kamil Elwawi, Ashwin Gopalakrishnan, Gharam Mansour and Thuan Nguyen

## Overview
The WebCrawler is a simple Java application designed to navigate web pages starting from a given URL, extracting links and visiting them up to a specified depth and link count. 

## Prerequisites
- Java 11 or higher
- Command Line Interface (CLI) or any Java IDE (such as IntelliJ IDEA or Eclipse)

## Features
- Crawls web pages to collect links starting from a specified URL.
- Dynamically asks the user to specify the maximum depth of crawling and the maximum number of links to crawl.
- Avoids revisiting the same link to prevent infinite loops.
- Asks for inputs of maximum depth and maximum number of links to crawl.


## Usage
To run the WebCrawler, use the following command from the command line:

java WebCrawler.java




## Installation
No installation is needed other than having Java installed on your machine. Clone this repository or download the source code to your local machine.

```bash
git clone https://github.com/yourusername/webcrawler.git
cd webcrawler


